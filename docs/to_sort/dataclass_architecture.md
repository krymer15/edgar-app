# Safe Harbor EDGAR AI App â€“ Dataclass Pipeline Map

This document outlines the key dataclasses used throughout the Safe Harbor EDGAR AI App ingestion, parsing, and embedding pipelines. Each dataclass represents a structured unit of data passed between pipeline stages, ensuring traceability, strong typing, and consistent metadata handling across collectors, downloaders, writers, and parsers.

Reference: https://chatgpt.com/c/681f8e70-c75c-800c-a149-4601f31f0a4f?model=gpt-4o

---

## ğŸ” Pipeline Dataclass Map

| Stage          | Input Class           | Output Class           | Description                                                                 |
|----------------|-----------------------|-------------------------|-----------------------------------------------------------------------------|
| Collector      | â€”                     | `FilingMetadata`        | Extracts CIK, accession, form type, and filing dates from SEC index.       |
| Downloader     | `FilingMetadata`      | `DownloadedDocument`    | Downloads raw SEC content via metadata URL.                                |
| Raw Writer     | `DownloadedDocument`  | `RawDocument`           | Saves raw SGML/HTML/XBRL files to disk.                                    |
| Cleaner        | `RawDocument`         | `CleanedDocument`       | Normalizes HTML content, strips tags, and handles whitespace.              |
| Parser         | `CleanedDocument`     | `ParsedDocument`        | Extracts structured content, segments, and flags from filings.             |
| Chunker        | `ParsedDocument`      | `ParsedChunk`           | Splits large text into embedding-ready segments.                           |
| Embedder       | `ParsedChunk`         | `ParsedChunk` (updated) | Appends vector embeddings for semantic search.                             |
| DB Writer      | All above             | ORM Models              | Maps and stores the dataclasses into SQLAlchemy tables.                    |

---

## ğŸ§± Core Dataclasses (Defined in `models/dataclasses/`)

### `FilingMetadata`
Used by: **Collector â†’ Downloader â†’ DB Writer**

```python
@dataclass
class FilingMetadata:
    accession_number: str
    cik: str
    form_type: str
    filing_date: date
    filing_url: Optional[str] = None
    period_of_report: Optional[date] = None
    accepted_datetime: Optional[datetime] = None
```
### `DownloadedDocument`
Used by: **Downloader â†’ Raw Writer**

```python
@dataclass
class DownloadedDocument:
    accession_number: str
    cik: str
    form_type: str
    source_url: str
    raw_bytes: bytes
    doc_type: str
    sequence: Optional[int] = None
    is_primary: bool = False
    is_exhibit: bool = False
    content_type: Optional[str] = None
    fetch_timestamp: datetime
```

### `RawDocument`
Used by: `Raw Writer â†’ Cleaner`

```python
@dataclass
class RawDocument:
    accession_number: str
    cik: str
    form_type: str
    filename: str
    local_path: Optional[str]
    source_url: str
    doc_type: str
    is_primary: bool
    is_exhibit: bool
    content_type: Optional[str]
```

### `CleanedDocument`
Used by: `Cleaner â†’ Parser`

```python
@dataclass
class CleanedDocument:
    accession_number: str
    cik: str
    doc_type: str
    local_path: str
    cleaned_text: str
    sequence: Optional[int]
    cleaned_timestamp: datetime
```

### `ParsedDocument`
Used by: `Parser â†’ Chunker`

```python
@dataclass
class ParsedDocument:
    accession_number: str
    cik: str
    form_type: str
    doc_type: str
    sequence: Optional[int]
    text: str
    local_path: Optional[str]
```

### `ParsedChunk`
Used by: `Chunker â†’ Embedder â†’ DB Writer`

```python
@dataclass
class ParsedChunk:
    accession_number: str
    cik: str
    form_type: str
    chunk_id: str
    text: str
    metadata: Dict[str, Optional[str]]
    embedding: Optional[List[float]]
```

## ğŸ§© Composite Wrappers
These classes group multiple dataclasses into coherent filing-level units:

### Filing
```python
@dataclass
class Filing:
    header: FilingHeader
    company: CompanyInfo
    raw_documents: List[RawDocument]
    parsed_documents: List[ParsedDocument]
    chunks: List[ParsedChunk]
    extracted_metrics: Dict[str, float]
```

### ParsedFiling
```python
@dataclass
class ParsedFiling:
    header: FilingHeader
    company: CompanyInfo
    documents: List[ParsedDocument]
    chunks: List[ParsedChunk]
    extracted_metrics: Dict[str, float]
```

## ğŸ§ª Specialized Subclass Guidance by Pipeline Stage

| Stage      | Specialization Required? | Reason                                                                                  |
| ---------- | ------------------------ | --------------------------------------------------------------------------------------- |
| Downloader | Rarely                   | Only needed for forms like Form 4 XML with unique URL construction.                     |
| Cleaner    | Sometimes                | Varies for forms with unique embedded scripts or formatting (e.g., 10-K vs. 8-K).       |
| Parser     | Frequently               | Core stage for subclassing due to schema differences across forms (e.g., 10-Q, 4, S-1). |
| Writer     | Rarely                   | Use subclass only when writing to non-standard DB schema (e.g., `form4_transactions`).  |
| Embedder   | No                       | Inputs should always be normalized `ParsedChunk`s.                                      |

## ğŸ§¬ Folder Design for Parser Subclassing
Each parsers/[type]/ folder should include specialized helpers:

```bash
parsers/
â””â”€â”€ html/
    â”œâ”€â”€ html_parser.py          # Orchestrates logic and maps DOM â†’ dataclasses
    â”œâ”€â”€ html_utils.py           # Node traversal, tag stripping, cleaning
    â”œâ”€â”€ html_selectors.py       # Reusable CSS/XPath selectors
    â”œâ”€â”€ html_models.py          # Section, Table, Link dataclasses
    â”œâ”€â”€ html_exceptions.py      # Custom exceptions (e.g., MissingElementError)
    â”œâ”€â”€ html_chunker.py         # Section-by-header splitting for chunking
    â””â”€â”€ html_schema.py          # Schema rules: DOM â†’ structured dataclass
```
Replicate this structure in `parsers/sgml/`, `parsers/xbrl/`, etc., to cleanly isolate form-specific logic and make parsers easily swappable.

## âœ… Best Practices
1. Use `dataclass` for all structured intermediates.
- Maintain immutability and type-checking benefits.

2. Preserve `accession_number`, `cik`, and `form_type` across all stages.
-These identifiers unify stages and serve as primary keys in Postgres.

3. Capture SEC sequence order.
Use `sequence: Optional[int]` on document classes.

4. Track provenance and file status.
- Add `source_type`, `is_exhibit`, and `accessible` fields for transparency.

5. Lean on helper modules in `parsers/[form_type]/`.
- Keep your main parser focused on logic orchestration.

6. ORM Models mirror these dataclasses.
- Define your SQLAlchemy tables in `models/orm_models/`.

## ğŸ“ File Tree Reference

```bash
models/
â””â”€â”€ dataclasses/
    â”œâ”€â”€ filing_metadata.py
    â”œâ”€â”€ filing_document.py
    â”œâ”€â”€ parsed_document.py
    â”œâ”€â”€ parsed_chunk.py
    â””â”€â”€ raw_document.py
```

Subclasses for parsed forms exist in:
```bash
parsers/forms/
â”œâ”€â”€ form4_parser.py
â”œâ”€â”€ form10k_parser.py
...
```

## ğŸ“Œ Summary
A disciplined dataclass architecture powers your ingestion pipelines with:

- ğŸ”„ Reusability between pipelines
- ğŸ”’ Data integrity and schema clarity
- âš™ï¸ Easy subclassing for form specialization
- ğŸ§  Compatibility with RAG & vector embeddings
- ğŸ¦ Seamless Postgres persistence

This structure ensures long-term scalability and auditability as the Safe Harbor EDGAR AI Platform evolves.

## Summary of benefits
- Consistency & integrity â€“ Every stage has a well-defined schema; no more loose tuples or missing flags.
- Full SEC coverage â€“ Youâ€™ll capture sequence numbers, filing vs. period dates, accepted timestamps, and MIME types.
- Scalability â€“ Local paths and timestamps let your writers and downstream analytics know exactly which version theyâ€™re operating on.
- Embedding readiness â€“ `ParsedChunk` is the canonical unit for your RAG/pgvector layer.

## Recommendations at a Glance
1. Augment SEC metadata
- `period_of_report: date`
- `accepted_datetime: datetime`
- `sequence: int` and `description: str` on `RawDocument`

2. Strengthen typing
- Use Python `Enum` for ``doc_type`/`source_type`.
- Validate accession formats (e.g. via regex) at ingestion.

3. Optimize for scale
- Add DB indexes on (`filing_date`), (`form_type`), and any new timestamp fields.
- Plan table partitioning if you ingest 10+ years of daily filings.

With those tweaks, youâ€™ll capture the full richness of the SECâ€™s filing metadata, improve data integrity, and ensure high-performance querying as your â€œSafe Harbor Edgar AI Appâ€ grows.